{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgAs4PCiVg8S",
        "outputId": "555752cd-b7c2-4bee-fc48-835780ba4e29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-05-01 21:27:23--  https://norvig.com/big.txt\n",
            "Resolving norvig.com (norvig.com)... 158.106.138.13\n",
            "Connecting to norvig.com (norvig.com)|158.106.138.13|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6488666 (6.2M) [text/plain]\n",
            "Saving to: ‘big.txt’\n",
            "\n",
            "big.txt             100%[===================>]   6.19M  11.3MB/s    in 0.5s    \n",
            "\n",
            "2023-05-01 21:27:24 (11.3 MB/s) - ‘big.txt’ saved [6488666/6488666]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#!wget https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\n",
        "#filename = 't8.shakespeare.txt'\n",
        "!wget https://norvig.com/big.txt\n",
        "filename = 'big.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pit3F6_0WDX6"
      },
      "outputs": [],
      "source": [
        "with open(filename, 'r', encoding='utf-8') as f:\n",
        "  text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpR7ZI2aWUuH",
        "outputId": "0a05cdbd-93d3-4aa3-a4b2-0f90aa39db6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of text:  6488666\n"
          ]
        }
      ],
      "source": [
        "print(\"Length of text: \", len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmDCP67-WhEZ",
        "outputId": "fd9352da-1f46-4d85-96c2-6da16b60df54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Project Gutenberg EBook of The Adventures of Sherlock Holmes\n",
            "by Sir Arthur Conan Doyle\n",
            "(#15 in our series by Sir Arthur Conan Doyle)\n",
            "\n",
            "Copyright laws are changing all over the world. Be sure to check the\n",
            "copyright laws for your country before downloading or redistributing\n",
            "this or any other Project Gutenberg eBook.\n",
            "\n",
            "This header should be the first thing seen when viewing this Project\n",
            "Gutenberg file.  Please do not remove it.  Do not change or edit the\n",
            "header without written permission.\n",
            "\n",
            "Please read the \"legal small print,\" and other information about the\n",
            "eBook and Project Gutenberg at the bottom of this file.  Included is\n",
            "important information about your specific rights and restrictions in\n",
            "how the file may be used.  You can also find out about how to make a\n",
            "donation to Project Gutenberg, and how to get involved.\n",
            "\n",
            "\n",
            "**Welcome To The World of Free Plain Vanilla Electronic Texts**\n",
            "\n",
            "**eBooks Readable By Both Humans and By Computers, Since 1971**\n",
            "\n",
            "*****These eBooks Were Prepared By Thousan\n"
          ]
        }
      ],
      "source": [
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mq1THQkRXjDY",
        "outputId": "4caa5fbc-997b-4ba4-bbe5-b8e37aa42627"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\n",
            " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]^_abcdefghijklmnopqrstuvwxyz|~\n",
            "size:  93\n"
          ]
        }
      ],
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(\"size: \", vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMSMpwu0YMcA",
        "outputId": "10658c0d-2195-4e8f-95c8-c36b8558aaba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[43, 2, 76, 79, 86, 69, 2, 89, 79, 85]\n",
            "I love you\n"
          ]
        }
      ],
      "source": [
        "stoi = { ch:i for i, ch in enumerate(chars) }\n",
        "itos = { i:ch for i, ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "print(encode(\"I love you\"))\n",
        "print(decode(encode(\"I love you\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b15W9wHJaB4W",
        "outputId": "16493c13-5632-4fe4-b8c3-b55522985007"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([6488666]) torch.int64\n",
            "tensor([54, 72, 69,  2, 50, 82, 79, 74, 69, 67, 84,  2, 41, 85, 84, 69, 78, 66,\n",
            "        69, 82, 71,  2, 39, 36, 79, 79, 75,  2, 79, 70,  2, 54, 72, 69,  2, 35,\n",
            "        68, 86, 69, 78, 84, 85, 82, 69, 83,  2, 79, 70,  2, 53, 72, 69, 82, 76,\n",
            "        79, 67, 75,  2, 42, 79, 76, 77, 69, 83,  1, 66, 89,  2, 53, 73, 82,  2,\n",
            "        35, 82, 84, 72, 85, 82,  2, 37, 79, 78, 65, 78,  2, 38, 79, 89, 76, 69,\n",
            "         1, 10,  5, 19, 23,  2, 73, 78,  2, 79, 85, 82,  2, 83, 69, 82, 73, 69,\n",
            "        83,  2, 66, 89,  2, 53, 73, 82,  2, 35, 82, 84, 72, 85, 82,  2, 37, 79,\n",
            "        78, 65, 78,  2, 38, 79, 89, 76, 69, 11,  1,  1, 37, 79, 80, 89, 82, 73,\n",
            "        71, 72, 84,  2, 76, 65, 87, 83,  2, 65, 82, 69,  2, 67, 72, 65, 78, 71,\n",
            "        73, 78, 71,  2, 65, 76, 76,  2, 79, 86, 69, 82,  2, 84, 72, 69,  2, 87,\n",
            "        79, 82, 76, 68, 16,  2, 36, 69,  2, 83, 85, 82, 69,  2, 84, 79,  2, 67,\n",
            "        72, 69, 67, 75,  2, 84, 72, 69,  1, 67, 79, 80, 89, 82, 73, 71, 72, 84,\n",
            "         2, 76, 65, 87, 83,  2, 70, 79, 82,  2, 89, 79, 85, 82,  2, 67, 79, 85,\n",
            "        78, 84, 82, 89,  2, 66, 69, 70, 79, 82, 69,  2, 68, 79, 87, 78, 76, 79,\n",
            "        65, 68, 73, 78, 71,  2, 79, 82,  2, 82, 69, 68, 73, 83, 84, 82, 73, 66,\n",
            "        85, 84, 73, 78, 71,  1, 84, 72, 73, 83,  2, 79, 82,  2, 65, 78, 89,  2,\n",
            "        79, 84, 72, 69, 82,  2, 50, 82, 79, 74, 69, 67, 84,  2, 41, 85, 84, 69,\n",
            "        78, 66, 69, 82, 71,  2, 69, 36, 79, 79, 75, 16,  1,  1, 54, 72, 73, 83,\n",
            "         2, 72, 69, 65, 68, 69, 82,  2, 83, 72, 79, 85, 76, 68,  2, 66, 69,  2,\n",
            "        84, 72, 69,  2, 70, 73, 82, 83, 84,  2, 84, 72, 73, 78, 71,  2, 83, 69,\n",
            "        69, 78,  2, 87, 72, 69, 78,  2, 86, 73, 69, 87, 73, 78, 71,  2, 84, 72,\n",
            "        73, 83,  2, 50, 82, 79, 74, 69, 67, 84,  1, 41, 85, 84, 69, 78, 66, 69,\n",
            "        82, 71,  2, 70, 73, 76, 69, 16,  2,  2, 50, 76, 69, 65, 83, 69,  2, 68,\n",
            "        79,  2, 78, 79, 84,  2, 82, 69, 77, 79, 86, 69,  2, 73, 84, 16,  2,  2,\n",
            "        38, 79,  2, 78, 79, 84,  2, 67, 72, 65, 78, 71, 69,  2, 79, 82,  2, 69,\n",
            "        68, 73, 84,  2, 84, 72, 69,  1, 72, 69, 65, 68, 69, 82,  2, 87, 73, 84,\n",
            "        72, 79, 85, 84,  2, 87, 82, 73, 84, 84, 69, 78,  2, 80, 69, 82, 77, 73,\n",
            "        83, 83, 73, 79, 78, 16,  1,  1, 50, 76, 69, 65, 83, 69,  2, 82, 69, 65,\n",
            "        68,  2, 84, 72, 69,  2,  4, 76, 69, 71, 65, 76,  2, 83, 77, 65, 76, 76,\n",
            "         2, 80, 82, 73, 78, 84, 14,  4,  2, 65, 78, 68,  2, 79, 84, 72, 69, 82,\n",
            "         2, 73, 78, 70, 79, 82, 77, 65, 84, 73, 79, 78,  2, 65, 66, 79, 85, 84,\n",
            "         2, 84, 72, 69,  1, 69, 36, 79, 79, 75,  2, 65, 78, 68,  2, 50, 82, 79,\n",
            "        74, 69, 67, 84,  2, 41, 85, 84, 69, 78, 66, 69, 82, 71,  2, 65, 84,  2,\n",
            "        84, 72, 69,  2, 66, 79, 84, 84, 79, 77,  2, 79, 70,  2, 84, 72, 73, 83,\n",
            "         2, 70, 73, 76, 69, 16,  2,  2, 43, 78, 67, 76, 85, 68, 69, 68,  2, 73,\n",
            "        83,  1, 73, 77, 80, 79, 82, 84, 65, 78, 84,  2, 73, 78, 70, 79, 82, 77,\n",
            "        65, 84, 73, 79, 78,  2, 65, 66, 79, 85, 84,  2, 89, 79, 85, 82,  2, 83,\n",
            "        80, 69, 67, 73, 70, 73, 67,  2, 82, 73, 71, 72, 84, 83,  2, 65, 78, 68,\n",
            "         2, 82, 69, 83, 84, 82, 73, 67, 84, 73, 79, 78, 83,  2, 73, 78,  1, 72,\n",
            "        79, 87,  2, 84, 72, 69,  2, 70, 73, 76, 69,  2, 77, 65, 89,  2, 66, 69,\n",
            "         2, 85, 83, 69, 68, 16,  2,  2, 59, 79, 85,  2, 67, 65, 78,  2, 65, 76,\n",
            "        83, 79,  2, 70, 73, 78, 68,  2, 79, 85, 84,  2, 65, 66, 79, 85, 84,  2,\n",
            "        72, 79, 87,  2, 84, 79,  2, 77, 65, 75, 69,  2, 65,  1, 68, 79, 78, 65,\n",
            "        84, 73, 79, 78,  2, 84, 79,  2, 50, 82, 79, 74, 69, 67, 84,  2, 41, 85,\n",
            "        84, 69, 78, 66, 69, 82, 71, 14,  2, 65, 78, 68,  2, 72, 79, 87,  2, 84,\n",
            "        79,  2, 71, 69, 84,  2, 73, 78, 86, 79, 76, 86, 69, 68, 16,  1,  1,  1,\n",
            "        12, 12, 57, 69, 76, 67, 79, 77, 69,  2, 54, 79,  2, 54, 72, 69,  2, 57,\n",
            "        79, 82, 76, 68,  2, 79, 70,  2, 40, 82, 69, 69,  2, 50, 76, 65, 73, 78,\n",
            "         2, 56, 65, 78, 73, 76, 76, 65,  2, 39, 76, 69, 67, 84, 82, 79, 78, 73,\n",
            "        67,  2, 54, 69, 88, 84, 83, 12, 12,  1,  1, 12, 12, 69, 36, 79, 79, 75,\n",
            "        83,  2, 52, 69, 65, 68, 65, 66, 76, 69,  2, 36, 89,  2, 36, 79, 84, 72,\n",
            "         2, 42, 85, 77, 65, 78, 83,  2, 65, 78, 68,  2, 36, 89,  2, 37, 79, 77,\n",
            "        80, 85, 84, 69, 82, 83, 14,  2, 53, 73, 78, 67, 69,  2, 19, 27, 25, 19,\n",
            "        12, 12,  1,  1, 12, 12, 12, 12, 12, 54, 72, 69, 83, 69,  2, 69, 36, 79,\n",
            "        79, 75, 83,  2, 57, 69, 82, 69,  2, 50, 82, 69, 80, 65, 82, 69, 68,  2,\n",
            "        36, 89,  2, 54, 72, 79, 85, 83, 65, 78])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRr_btR0au2H"
      },
      "outputs": [],
      "source": [
        "# Setting up train and validation sets\n",
        "split = int(0.9 * len(data))\n",
        "train_data = data[:split]\n",
        "val_data = data[split:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cn-H8mv9b8pu",
        "outputId": "0d061378-3703-45da-b00e-ea3a3b872ae1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([54, 72, 69,  2, 50, 82, 79, 74, 69])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFRFkJFIcMgu",
        "outputId": "f8e76822-c412-4854-9260-d01210d0ab21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "When input is tensor([54]) then the target is 72\n",
            "When input is tensor([54, 72]) then the target is 69\n",
            "When input is tensor([54, 72, 69]) then the target is 2\n",
            "When input is tensor([54, 72, 69,  2]) then the target is 50\n",
            "When input is tensor([54, 72, 69,  2, 50]) then the target is 82\n",
            "When input is tensor([54, 72, 69,  2, 50, 82]) then the target is 79\n",
            "When input is tensor([54, 72, 69,  2, 50, 82, 79]) then the target is 74\n",
            "When input is tensor([54, 72, 69,  2, 50, 82, 79, 74]) then the target is 69\n"
          ]
        }
      ],
      "source": [
        "# illustrating training on a block\n",
        "# we take a sequence of chars (len 1 to block_size), and see what appears next\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:1+block_size]\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f\"When input is {context} then the target is {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDvqbDD1dcHA",
        "outputId": "d25ef934-1d6e-4b67-933f-f4298bbb6469"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[68, 73, 86, 69, 82, 83, 73, 84],\n",
            "        [82, 69, 83, 69, 77, 66, 76, 73],\n",
            "        [68,  2, 87, 72, 65, 84,  2, 89],\n",
            "        [79, 79, 84, 83, 84, 69, 80, 83]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[73, 86, 69, 82, 83, 73, 84, 89],\n",
            "        [69, 83, 69, 77, 66, 76, 73, 78],\n",
            "        [ 2, 87, 72, 65, 84,  2, 89, 79],\n",
            "        [79, 84, 83, 84, 69, 80, 83,  2]])\n",
            "====\n",
            "When input is tensor([68]) then the target is 73\n",
            "When input is tensor([68, 73]) then the target is 86\n",
            "When input is tensor([68, 73, 86]) then the target is 69\n",
            "When input is tensor([68, 73, 86, 69]) then the target is 82\n",
            "When input is tensor([68, 73, 86, 69, 82]) then the target is 83\n",
            "When input is tensor([68, 73, 86, 69, 82, 83]) then the target is 73\n",
            "When input is tensor([68, 73, 86, 69, 82, 83, 73]) then the target is 84\n",
            "When input is tensor([68, 73, 86, 69, 82, 83, 73, 84]) then the target is 89\n",
            "When input is tensor([82]) then the target is 69\n",
            "When input is tensor([82, 69]) then the target is 83\n",
            "When input is tensor([82, 69, 83]) then the target is 69\n",
            "When input is tensor([82, 69, 83, 69]) then the target is 77\n",
            "When input is tensor([82, 69, 83, 69, 77]) then the target is 66\n",
            "When input is tensor([82, 69, 83, 69, 77, 66]) then the target is 76\n",
            "When input is tensor([82, 69, 83, 69, 77, 66, 76]) then the target is 73\n",
            "When input is tensor([82, 69, 83, 69, 77, 66, 76, 73]) then the target is 78\n",
            "When input is tensor([68]) then the target is 2\n",
            "When input is tensor([68,  2]) then the target is 87\n",
            "When input is tensor([68,  2, 87]) then the target is 72\n",
            "When input is tensor([68,  2, 87, 72]) then the target is 65\n",
            "When input is tensor([68,  2, 87, 72, 65]) then the target is 84\n",
            "When input is tensor([68,  2, 87, 72, 65, 84]) then the target is 2\n",
            "When input is tensor([68,  2, 87, 72, 65, 84,  2]) then the target is 89\n",
            "When input is tensor([68,  2, 87, 72, 65, 84,  2, 89]) then the target is 79\n",
            "When input is tensor([79]) then the target is 79\n",
            "When input is tensor([79, 79]) then the target is 84\n",
            "When input is tensor([79, 79, 84]) then the target is 83\n",
            "When input is tensor([79, 79, 84, 83]) then the target is 84\n",
            "When input is tensor([79, 79, 84, 83, 84]) then the target is 69\n",
            "When input is tensor([79, 79, 84, 83, 84, 69]) then the target is 80\n",
            "When input is tensor([79, 79, 84, 83, 84, 69, 80]) then the target is 83\n",
            "When input is tensor([79, 79, 84, 83, 84, 69, 80, 83]) then the target is 2\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(989)\n",
        "batch_size = 4 # how many independent sequences to process in parallel\n",
        "block_size = 8\n",
        "\n",
        "def get_batch(split):\n",
        "  # generate a small batch of data of inputs x and targets y\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
        "  return x, y\n",
        "\n",
        "# here's what gets fed into the neural network\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "print('====')\n",
        "\n",
        "# illustrating the training again\n",
        "for b in range(batch_size):\n",
        "  for t in range(block_size):\n",
        "    context = xb[b, :t+1]\n",
        "    target = yb[b, t]\n",
        "    print(f\"When input is {context} then the target is {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NA3lMucMg3dL",
        "outputId": "5032a741-dfc1-4eb4-e877-0e3edadf36fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 93])\n",
            "tensor(4.9968, grad_fn=<NllLossBackward0>)\n",
            "\tjgx5-jl14C1;Mr;v~(feQ<Vm=&=iRcP#I+M%zCeyld8Q0+9]k2:!k2.R\n",
            "%,gE]oz(\t5sn3cbUSB>I4yZ,eMG()ounv/k~kOK).h(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "torch.manual_seed(989)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(me, vocab_size):\n",
        "    super().__init__()\n",
        "    # each token directly reads off the logits for the next token from a lookup table\n",
        "    me.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def forward(me, idx, targets=None):\n",
        "    # idx and targets are both (B,T) tensor of integers\n",
        "    logits = me.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      # reshape the matrix to comply to pytorch's spec\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    return logits, loss\n",
        "  \n",
        "  def generate(me, idx, max_new_tokens):\n",
        "    # idx is (B, T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "      # get the prediction\n",
        "      logits, loss = me(idx)\n",
        "      # focus only on the last time step\n",
        "      logits = logits[:, -1, :] # becomes (B,C)\n",
        "      probs = F.softmax(logits, dim=-1) # (B,C)\n",
        "      # sample from the distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B,T+1)\n",
        "    return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "# try generating, which will output garbage since we haven't trained\n",
        "# but at least we can see that the plumbings work\n",
        "gen_result = m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)\n",
        "print(decode(gen_result[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Snsq4PLjZJHS"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1.e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6S4DIFSrZafV",
        "outputId": "cf062642-7bd0-4556-a1d8-4df7a1acaa10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.497157573699951\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "\n",
        "# train the model\n",
        "for steps in range(10000):\n",
        "  #sample a batch of data\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  #evaluate the loss\n",
        "  logits, loss = m(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZiygd1YaX2O",
        "outputId": "e6a3a97f-edff-4878-d466-e2c66d8be0af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t).\n",
            "an d d ak th\n",
            "\n",
            "on or ty tar heran cuthov#Th, n aremandy MarsqurenKr e. ban\n",
            "torunes,Ohomot\n",
            "r an thett bymi'ssofof I tins omap co:\n",
            "cett wng avewe ueaxcedgend Did maler, oly te akwe mon\n",
            "\"\n",
            "\n",
            "bo Hexpweait erit to de\n",
            " abn thevice, s g winov..  st ke, a Prous d h cowh sestok, his ad ca. r. ine t thing indind If audlisht d w$\"t d Bodjone thin an ncind inamusase ofans ofiRUn f whem brgre\n",
            "Thatiteplof\n",
            "Kull \n"
          ]
        }
      ],
      "source": [
        "# retry the generation with trained model\n",
        "gen_result = m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=400)\n",
        "print(decode(gen_result[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9nA87UTD51S",
        "outputId": "c6ed769f-e6ad-46b9-cfb1-df8efb6cdd38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 8, 2])\n"
          ]
        }
      ],
      "source": [
        "# Mathematical trick in Self Attention\n",
        "\n",
        "# example, to make tokens get info from previous ones (but not future ones)\n",
        "torch.manual_seed(444)\n",
        "B, T, C = 4, 8, 2 # batch, time, channels\n",
        "x = torch.randn(B, T, C)\n",
        "print(x.shape)\n",
        "\n",
        "# one lossy way to do that is to average all previous tokens plus itself\n",
        "# x[b,t] = mean_{i<=t} x[b,i]\n",
        "# version 1: brute force with for-loops\n",
        "xbow = torch.zeros((B,T,C)) # x-bag-of-words\n",
        "for b in range(B):\n",
        "  for t in range(T):\n",
        "    xprev = x[b,:t+1] # (t, C)\n",
        "    xbow[b,t] = torch.mean(xprev, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BxX2HXoG5vo",
        "outputId": "26e6ad4a-1d88-44f7-f3aa-79696f3d501a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "original\n",
            " tensor([[[ 2.0291,  0.9115],\n",
            "         [ 0.2897, -1.8648],\n",
            "         [ 0.2171, -1.0556],\n",
            "         [ 1.8147,  1.5656],\n",
            "         [ 0.2540,  0.0917],\n",
            "         [-0.7039,  1.5540],\n",
            "         [-0.9957,  0.1390],\n",
            "         [-0.1338, -0.1376]],\n",
            "\n",
            "        [[ 1.1425, -0.4524],\n",
            "         [-0.7752, -1.6074],\n",
            "         [-0.3859, -0.0484],\n",
            "         [ 1.2081,  0.0876],\n",
            "         [-0.5788,  1.1616],\n",
            "         [ 0.7847, -0.2466],\n",
            "         [ 0.9276, -1.5267],\n",
            "         [ 0.7774, -0.0511]],\n",
            "\n",
            "        [[ 0.3798,  2.5237],\n",
            "         [ 0.8944, -0.4245],\n",
            "         [-0.0309,  0.6419],\n",
            "         [ 0.5361, -0.9341],\n",
            "         [ 1.1554,  1.1066],\n",
            "         [ 0.0873, -0.1423],\n",
            "         [-2.4982, -1.9596],\n",
            "         [ 0.4957,  2.2735]],\n",
            "\n",
            "        [[ 0.2789, -1.0112],\n",
            "         [-1.3280,  0.7159],\n",
            "         [ 1.9788,  0.6944],\n",
            "         [ 1.3840,  0.5109],\n",
            "         [ 1.2745,  1.0589],\n",
            "         [-0.5768, -0.0208],\n",
            "         [-1.0459,  1.3953],\n",
            "         [-0.9601,  0.6664]]])\n"
          ]
        }
      ],
      "source": [
        "print(\"original\\n\", x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLMqN5gfHRyc",
        "outputId": "13551bde-5396-49bf-9bad-c3cc0e66d073"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x-bag-of-words\n",
            " tensor([[ 0.2789, -1.0112],\n",
            "        [-0.5246, -0.1477],\n",
            "        [ 0.3099,  0.1330],\n",
            "        [ 0.5784,  0.2275],\n",
            "        [ 0.7176,  0.3938],\n",
            "        [ 0.5019,  0.3247],\n",
            "        [ 0.2808,  0.4776],\n",
            "        [ 0.1257,  0.5012]])\n"
          ]
        }
      ],
      "source": [
        "print(\"x-bag-of-words\\n\", xbow[3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vRvJwmrHmqI",
        "outputId": "dcb622a2-f616-4d3d-8c96-4db180e2828b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* tril a =\n",
            " tensor([[1., 0., 0.],\n",
            "        [1., 1., 0.],\n",
            "        [1., 1., 1.]])\n",
            "* tril a normalized =\n",
            " tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "* rand int b =\n",
            " tensor([[7., 7.],\n",
            "        [8., 2.],\n",
            "        [4., 9.]])\n",
            "* moving avg of b -> c =\n",
            " tensor([[7.0000, 7.0000],\n",
            "        [7.5000, 4.5000],\n",
            "        [6.3333, 6.0000]])\n"
          ]
        }
      ],
      "source": [
        "# the above works, but uses slow for-loops\n",
        "# version 2: improve with matrix multiplication trick\n",
        "a = torch.tril(torch.ones(3, 3)) # lower triangle section have ones\n",
        "print(\"* tril a =\\n\", a)\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "print(\"* tril a normalized =\\n\", a)\n",
        "b = torch.randint(1, 10, (3, 2)).float()\n",
        "print(\"* rand int b =\\n\", b)\n",
        "c = a @ b\n",
        "print(\"* moving avg of b -> c =\\n\", c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyAvxvRQLgDc",
        "outputId": "9c7bf0b2-bcc3-484c-b2b3-7f34acb31e23"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.2789, -1.0112],\n",
              "        [-0.5246, -0.1477],\n",
              "        [ 0.3099,  0.1330],\n",
              "        [ 0.5784,  0.2275],\n",
              "        [ 0.7176,  0.3938],\n",
              "        [ 0.5019,  0.3247],\n",
              "        [ 0.2808,  0.4776],\n",
              "        [ 0.1257,  0.5012]])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# now do the moving average on the weights\n",
        "wei = torch.tril(torch.ones(T,T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (B,T,T) @ (B,T,C) -> (B,T,C)\n",
        "assert(torch.allclose(xbow, xbow2)) # equals to xbow, with faster calculation\n",
        "xbow2[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_y9OS8xZWeDC",
        "outputId": "f506e905-ce6f-4cb6-f32a-09710c0ee8c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "print(wei) # same moving avg matrix multiplier, but created with softmax\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nq9ghYvYdziI",
        "outputId": "dcb06cae-a7e8-4ed6-921a-18a1f927e988"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# version 4: Self Attention!\n",
        "torch.manual_seed(123)\n",
        "B, T, C = 4, 8, 32 # batch, time, channels\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x) # (B,T,16)\n",
        "q = query(x) # (B,T,16)\n",
        "wei = q @ k.transpose(-2, -1) # (B,T,16) @(B,16,T) -> (B,T,T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "# wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "# out = wei @ x\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TW7Is1ImrxRg",
        "outputId": "3b91fa16-00d6-4091-8a56-43e244d7a882"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2461, 0.7539, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1788, 0.0721, 0.7490, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0198, 0.0119, 0.9663, 0.0021, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0868, 0.0224, 0.6254, 0.0866, 0.1788, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0674, 0.2265, 0.0070, 0.2752, 0.2119, 0.2120, 0.0000, 0.0000],\n",
              "        [0.0844, 0.0189, 0.6506, 0.0281, 0.0320, 0.1514, 0.0346, 0.0000],\n",
              "        [0.0332, 0.0784, 0.1998, 0.1428, 0.1057, 0.0981, 0.3181, 0.0240]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uoor_LdAurJG"
      },
      "source": [
        "Notes:\n",
        "* Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at eadh other and aggregating information with a wieghted sum from all nodes that point to them, with data-dependent weights.\n",
        "* There is no notion of space. Attention simply acts over a set of vectors. This is why we need to  positionally encode tokens.\n",
        "* Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
        "* In an \"incoder\" attention block just delete the single line that does making with tril, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modelling.\n",
        "* \"self attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
        "* \"Scaled\" attention additional deivides wei by 1/sqrt(head_size). This makes it so when input Q, K are unit variance, we will be unite variance too and Softmax will stay diffuse and not saturate too much."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmBOBsOjwJj8",
        "outputId": "86c17c04-7383-40d5-bbf8-69dee4966c48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 8, 8])\n",
            "tensor([[ 0.8493,  5.2974, -2.2423, -0.4562, -0.1847,  6.5117, -1.2332,  7.0318],\n",
            "        [ 0.8963,  0.1012,  2.8573,  1.0340, -1.8438, -3.6918,  1.7688, -2.6806],\n",
            "        [ 2.7227,  2.5355, -0.3369,  0.9038, -1.0929, -3.0553,  0.8732,  1.9568],\n",
            "        [ 2.7093,  3.1052,  3.2472,  4.1662, -0.2696, -1.7073,  0.9370,  0.1977],\n",
            "        [-1.7986,  3.4100,  0.4803, -1.9221, -1.5034,  4.7513, -3.4758, -3.9145],\n",
            "        [-3.2551, -9.6171, -0.4902,  1.0551,  0.5027, -0.2759,  3.1721, -3.6231],\n",
            "        [-3.3327,  5.2450,  1.2443, -1.9639, -4.3145,  4.5994, -3.4263,  0.1847],\n",
            "        [-6.1774,  6.2514,  0.5321, -3.1332, -4.9116, -2.0588,  0.4785,  1.9875]])\n",
            "0.9707003831863403, 0.9749341607093811, 13.919997215270996\n",
            "torch.Size([4, 8, 8])\n",
            "tensor([[ 0.2123,  1.3244, -0.5606, -0.1140, -0.0462,  1.6279, -0.3083,  1.7580],\n",
            "        [ 0.2241,  0.0253,  0.7143,  0.2585, -0.4609, -0.9229,  0.4422, -0.6701],\n",
            "        [ 0.6807,  0.6339, -0.0842,  0.2259, -0.2732, -0.7638,  0.2183,  0.4892],\n",
            "        [ 0.6773,  0.7763,  0.8118,  1.0416, -0.0674, -0.4268,  0.2342,  0.0494],\n",
            "        [-0.4497,  0.8525,  0.1201, -0.4805, -0.3759,  1.1878, -0.8690, -0.9786],\n",
            "        [-0.8138, -2.4043, -0.1226,  0.2638,  0.1257, -0.0690,  0.7930, -0.9058],\n",
            "        [-0.8332,  1.3113,  0.3111, -0.4910, -1.0786,  1.1499, -0.8566,  0.0462],\n",
            "        [-1.5443,  1.5629,  0.1330, -0.7833, -1.2279, -0.5147,  0.1196,  0.4969]])\n",
            "0.9707003831863403, 0.9749341607093811, 0.8699998259544373\n"
          ]
        }
      ],
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1)\n",
        "print(f\"{wei.shape}\\n{wei[0]}\\n{k.var()}, {q.var()}, {wei.var()}\")\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5 # scaled to 1/sqrt(head_size)\n",
        "print(f\"{wei.shape}\\n{wei[0]}\\n{k.var()}, {q.var()}, {wei.var()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fl2LVjctPOCO",
        "outputId": "526d45f7-055b-4772-c38a-7db319041c4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n",
            "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n"
          ]
        }
      ],
      "source": [
        "# why scaling is important\n",
        "# first with low variance, softmax works fine\n",
        "print(torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1))\n",
        "# second with high variance, softmax skews towards max\n",
        "print(torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "7def3786824c1036a398ff36813aa93068f56471c22d019359ef1cd14f1b3c4b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
